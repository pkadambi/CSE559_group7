{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETE\n",
    "Sequence-based Ensemble learning approach for TCR Epitope binding prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 15:29:01.407368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734128941.421263 1821575 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734128941.425383 1821575 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-13 15:29:01.441246: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import interp\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Concatenate, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191897, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epitope</th>\n",
       "      <th>tcr</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAAGIGILTV</td>\n",
       "      <td>CASSQEEGGGSWGNTIYF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EAAGIGILTV</td>\n",
       "      <td>CASSQGQLTDTQYF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EAAGIGILTV</td>\n",
       "      <td>CASSRTVGGPNEQF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EAAGIGILTV</td>\n",
       "      <td>CASSTGQGWGSF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAYGEPRKL</td>\n",
       "      <td>CASLAGQGYNEQF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epitope                 tcr  label\n",
       "0  EAAGIGILTV  CASSQEEGGGSWGNTIYF      1\n",
       "1  EAAGIGILTV      CASSQGQLTDTQYF      1\n",
       "2  EAAGIGILTV      CASSRTVGGPNEQF      1\n",
       "3  EAAGIGILTV        CASSTGQGWGSF      1\n",
       "4   SAYGEPRKL       CASLAGQGYNEQF      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path = '../data/BAP/tcr_split/train.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_file_path, header=None)\n",
    "train_df.columns = ['epitope', 'tcr', 'label']\n",
    "\n",
    "train_df['epitope'] = train_df['epitope'].str.upper()\n",
    "train_df['tcr'] = train_df['tcr'].str.upper()\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48042, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epitope</th>\n",
       "      <th>tcr</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAAGIGILTV</td>\n",
       "      <td>CASSQEGLAGASQYF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EAAGIGILTV</td>\n",
       "      <td>CASSQETDIVFNOPQHF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EAAGIGILTV</td>\n",
       "      <td>CSTDGQTGTGELF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVFDRKSDAK</td>\n",
       "      <td>CATGPRANTGELF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLCTLVAML</td>\n",
       "      <td>CASSSGELSAGELF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epitope                tcr  label\n",
       "0  EAAGIGILTV    CASSQEGLAGASQYF      1\n",
       "1  EAAGIGILTV  CASSQETDIVFNOPQHF      1\n",
       "2  EAAGIGILTV      CSTDGQTGTGELF      1\n",
       "3  AVFDRKSDAK      CATGPRANTGELF      1\n",
       "4   GLCTLVAML     CASSSGELSAGELF      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = '../data/BAP/tcr_split/test.csv'\n",
    "\n",
    "test_df = pd.read_csv(test_file_path, header=None)\n",
    "test_df.columns = ['epitope', 'tcr', 'label']\n",
    "\n",
    "test_df['epitope'] = test_df['epitope'].str.upper()\n",
    "test_df['tcr'] = test_df['tcr'].str.upper()\n",
    "\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df, k=2, remove_duplicate=False, min_tcrs_amount=1):\n",
    "        \n",
    "    if remove_duplicate:\n",
    "        df.drop_duplicates(subset=['epitope', 'tcr'], inplace=True)\n",
    "\n",
    "    # Group by epitope and filter based on min_tcrs_amount\n",
    "    epitope_counts = df.groupby('epitope').size()\n",
    "    valid_epitopes = epitope_counts[epitope_counts >= min_tcrs_amount].index\n",
    "    df = df[df['epitope'].isin(valid_epitopes)]\n",
    "\n",
    "    alphabet = ['A', 'B', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', \n",
    "                'X', 'B', 'J', 'Z', 'U', 'O']\n",
    "\n",
    "    # Generate all k-letter combinations (with repetition allowed)\n",
    "    k_letter_combinations = list(itertools.product(alphabet, repeat=k))\n",
    "\n",
    "    # Convert tuples to strings\n",
    "    k_letter_sequences = [''.join(comb) for comb in k_letter_combinations]\n",
    "    \n",
    "    print(f\"Distinct kmers: {len(k_letter_sequences)}\")\n",
    "    \n",
    "    # Convert list to dictionary\n",
    "    sequence_to_index_map = {sequence: index+1 for index, sequence in enumerate(k_letter_sequences)}\n",
    "    # print(sequence_to_index_map)\n",
    "\n",
    "    X = np.zeros((len(df), len(k_letter_sequences)*2 + 2 + 200))\n",
    "    y = df['label'].values\n",
    "\n",
    "    def get_kmer_indices(sequence, k, sequence_to_index_map):\n",
    "        indices = []\n",
    "        for i in range(len(sequence) - k + 1):\n",
    "            kmer = sequence[i:i+k]\n",
    "            if kmer in sequence_to_index_map:\n",
    "                indices.append(sequence_to_index_map[kmer])\n",
    "            else:\n",
    "                indices.append(-1)  # If the kmer is not found, use -1 (or any other placeholder)\n",
    "        return indices\n",
    "\n",
    "    # Process each row\n",
    "    for i, (_, row) in enumerate(df.iterrows()):\n",
    "        epitope = row['epitope']\n",
    "        tcr = row['tcr']\n",
    "        \n",
    "        # Get k-mer indices for epitope and tcr\n",
    "        epitope_indices = get_kmer_indices(epitope, k, sequence_to_index_map)\n",
    "        tcr_indices = get_kmer_indices(tcr, k, sequence_to_index_map) \n",
    "        \n",
    "        # Pad sequences to length 100\n",
    "        epitope_indices_padded = epitope_indices[:100] + [0] * (100 - len(epitope_indices))\n",
    "        tcr_indices_padded = tcr_indices[:100] + [0] * (100 - len(tcr_indices))\n",
    "\n",
    "        # Initialize the count array\n",
    "        epitope_counts = np.zeros(len(k_letter_sequences), dtype=int)\n",
    "\n",
    "        # Lambda function to process the epitope string in chunks of size k and update counts\n",
    "        process_kmers_epitope = lambda epitope: [\n",
    "            epitope_counts.__setitem__(sequence_to_index_map[epitope[j:j + k]], epitope_counts[sequence_to_index_map[epitope[j:j + k]]] + 1)\n",
    "            for j in range(len(epitope) - k + 1) \n",
    "        ]\n",
    "        \n",
    "        process_kmers_epitope(epitope)\n",
    "        epitope_counts = np.append(epitope_counts, len(epitope))\n",
    "\n",
    "        # Initialize the count array\n",
    "        tcr_counts = np.zeros(len(k_letter_sequences), dtype=int)\n",
    "\n",
    "        # Lambda function to process the epitope string in chunks of size k and update counts\n",
    "        process_kmers_tcr = lambda tcr: [\n",
    "            tcr_counts.__setitem__(sequence_to_index_map[tcr[j:j + k]], tcr_counts[sequence_to_index_map[tcr[j:j + k]]] + 1)\n",
    "            for j in range(len(tcr) - k + 1) \n",
    "        ]\n",
    "        \n",
    "        process_kmers_tcr(tcr)    \n",
    "        tcr_counts = np.append(tcr_counts, len(tcr))\n",
    "        \n",
    "        X[i] = np.concatenate([np.array(epitope_indices_padded), np.array(tcr_indices_padded), epitope_counts, tcr_counts])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct kmers: 729\n",
      "(191897, 1660)\n"
     ]
    }
   ],
   "source": [
    "X, y = data_preprocess(train_df, k=2, min_tcrs_amount=1)\n",
    "print(X.shape)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct kmers: 729\n",
      "(48042, 1660)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = data_preprocess(test_df, k=2, min_tcrs_amount=1)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=200)\n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_val_pca = pca.transform(X_val_scaled)\n",
    "# X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not going with PCA as the results on test data is poor\n",
    "\n",
    "X_train_pca = X_train_scaled\n",
    "X_val_pca = X_val_scaled\n",
    "X_test_pca = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.read_csv('./embeddings/X_features_epi_split_train.csv').values\n",
    "# y = pd.read_csv('./embeddings/y_labels_epi_split_train.csv')['label'].values\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X).to_csv('./embeddings/epi_split_train_X_k2.csv', index=False)\n",
    "# pd.DataFrame(y, columns=['label']).to_csv('./embeddings/epi_split_train_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.86678348326244\n",
      "Validation Accuracy: 0.7272016675351746\n"
     ]
    }
   ],
   "source": [
    "train_dmatrix = xgb.DMatrix(X_train_pca, label=y_train)\n",
    "val_dmatrix = xgb.DMatrix(X_val_pca)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  \n",
    "    'eval_metric': 'logloss',       \n",
    "    'max_depth': 7,                \n",
    "    'eta': 0.2,                    \n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train model\n",
    "xgb_model = xgb.train(params, train_dmatrix, num_boost_round=500)\n",
    "\n",
    "y_pred_proba_train = xgb_model.predict(train_dmatrix)\n",
    "y_pred_train = [1 if prob > 0.5 else 0 for prob in y_pred_proba_train]\n",
    "\n",
    "y_pred_proba_val = xgb_model.predict(val_dmatrix)\n",
    "y_pred_val = [1 if prob > 0.5 else 0 for prob in y_pred_proba_val]\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.3769           0.0087           31.37s\n",
      "         2           1.3601           0.0163           30.97s\n",
      "         3           1.3457           0.0158           30.30s\n",
      "         4           1.3349           0.0095           29.84s\n",
      "         5           1.3241           0.0115           29.38s\n",
      "         6           1.3145           0.0082           28.84s\n",
      "         7           1.3046           0.0100           28.24s\n",
      "         8           1.2920           0.0133           27.83s\n",
      "         9           1.2809           0.0085           27.42s\n",
      "        10           1.2730           0.0095           26.94s\n",
      "        20           1.2151           0.0005           22.39s\n",
      "        30           1.1790           0.0108           17.98s\n",
      "        40           1.1533           0.0069           13.44s\n",
      "        50           1.1360           0.0014            8.97s\n",
      "        60           1.1181          -0.0060            4.50s\n",
      "        70           1.1041          -0.0022            0.00s\n",
      "Training Accuracy: 0.7200440342111949\n",
      "Validation Accuracy: 0.6956487754038562\n"
     ]
    }
   ],
   "source": [
    "gb_model = GradientBoostingClassifier(learning_rate=0.1,\n",
    "                                        min_samples_leaf=20, max_features='sqrt', subsample=0.8,\n",
    "                                        random_state=42, n_estimators=70, max_depth=11,\n",
    "                                        min_samples_split=60, loss=\"log_loss\", verbose=1\n",
    "                                        )\n",
    "\n",
    "gb_model.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred_train = gb_model.predict(X_train_pca)\n",
    "y_pred_val = gb_model.predict(X_val_pca)\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734129138.583682 1821575 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22398 MB memory:  -> device: 0, name: NVIDIA A30, pci bus id: 0000:e2:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734129143.857438 1821847 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 16ms/step - accuracy: 0.6114 - loss: 0.6473 - val_accuracy: 0.6793 - val_loss: 0.5832\n",
      "Epoch 2/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.6829 - loss: 0.5801 - val_accuracy: 0.6922 - val_loss: 0.5683\n",
      "Epoch 3/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.6950 - loss: 0.5618 - val_accuracy: 0.6978 - val_loss: 0.5609\n",
      "Epoch 4/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7117 - loss: 0.5446 - val_accuracy: 0.7018 - val_loss: 0.5507\n",
      "Epoch 5/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7149 - loss: 0.5349 - val_accuracy: 0.7041 - val_loss: 0.5521\n",
      "Epoch 6/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 16ms/step - accuracy: 0.7226 - loss: 0.5258 - val_accuracy: 0.7064 - val_loss: 0.5473\n",
      "Epoch 7/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7262 - loss: 0.5199 - val_accuracy: 0.7095 - val_loss: 0.5450\n",
      "Epoch 8/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 16ms/step - accuracy: 0.7329 - loss: 0.5108 - val_accuracy: 0.7084 - val_loss: 0.5441\n",
      "Epoch 9/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7367 - loss: 0.5041 - val_accuracy: 0.7097 - val_loss: 0.5424\n",
      "Epoch 10/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7447 - loss: 0.4958 - val_accuracy: 0.7088 - val_loss: 0.5437\n",
      "Epoch 11/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7465 - loss: 0.4895 - val_accuracy: 0.7117 - val_loss: 0.5410\n",
      "Epoch 12/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7484 - loss: 0.4852 - val_accuracy: 0.7110 - val_loss: 0.5410\n",
      "Epoch 13/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7534 - loss: 0.4801 - val_accuracy: 0.7097 - val_loss: 0.5452\n",
      "Epoch 14/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - accuracy: 0.7588 - loss: 0.4738 - val_accuracy: 0.7093 - val_loss: 0.5421\n",
      "Epoch 15/15\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.7598 - loss: 0.4736 - val_accuracy: 0.7098 - val_loss: 0.5421\n",
      "\u001b[1m4798/4798\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step\n",
      "Training Accuracy: 0.7813662330556225\n",
      "Validation Accuracy: 0.7077384054194893\n"
     ]
    }
   ],
   "source": [
    "# Split the features into temporal and non-temporal\n",
    "temporal_features = 200  # First 200 features are temporal\n",
    "X_train_temporal = X_train_pca[:, :temporal_features]  # Temporal features\n",
    "X_train_non_temporal = X_train_pca[:, temporal_features:]  # Non-temporal features\n",
    "\n",
    "X_val_temporal = X_val_pca[:, :temporal_features]  # Temporal features\n",
    "X_val_non_temporal = X_val_pca[:, temporal_features:]  # Non-temporal features\n",
    "\n",
    "# Reshape temporal features to be 3D [samples, time steps, features]\n",
    "X_train_temporal_reshaped = X_train_temporal.reshape((X_train_temporal.shape[0], 100, 2))\n",
    "X_val_temporal_reshaped = X_val_temporal.reshape((X_val_temporal.shape[0], 100, 2))  \n",
    "\n",
    "# Build the model\n",
    "temporal_input = Input(shape=(100, 2))  # Input for temporal features\n",
    "temporal_branch = LSTM(256, return_sequences=True)(temporal_input)  # LSTM for temporal features\n",
    "temporal_branch = Dropout(0.3)(temporal_branch)\n",
    "temporal_branch = LSTM(128, return_sequences=True)(temporal_branch)\n",
    "temporal_branch = Dropout(0.3)(temporal_branch)\n",
    "temporal_branch = LSTM(64)(temporal_branch)  # Final LSTM layer for temporal data\n",
    "\n",
    "# Non-temporal input\n",
    "non_temporal_input = Input(shape=(X_train_non_temporal.shape[1],))  # Input for non-temporal features\n",
    "non_temporal_branch = Dense(128, activation='relu')(non_temporal_input)\n",
    "non_temporal_branch = Dropout(0.3)(non_temporal_branch)\n",
    "non_temporal_branch = Dense(64, activation='relu')(non_temporal_branch)\n",
    "\n",
    "# Concatenate temporal and non-temporal branches\n",
    "merged = Concatenate()([temporal_branch, non_temporal_branch])\n",
    "\n",
    "# Final Dense layers\n",
    "dense_layer = Dense(128, activation='relu')(merged)\n",
    "dense_layer = Dropout(0.3)(dense_layer)\n",
    "dense_layer = Dense(64, activation='relu')(dense_layer)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "# Compile the model\n",
    "nn_model = Model(inputs=[temporal_input, non_temporal_input], outputs=output)\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit([X_train_temporal_reshaped, X_train_non_temporal], y_train,\n",
    "                       epochs=15,\n",
    "                       batch_size=64,\n",
    "                       validation_split=0.15,\n",
    "                       verbose=1)\n",
    "\n",
    "# Predictions\n",
    "y_pred_proba_train = nn_model.predict([X_train_temporal_reshaped, X_train_non_temporal])\n",
    "y_pred_train = [1 if prob > 0.5 else 0 for prob in y_pred_proba_train]\n",
    "\n",
    "y_pred_proba_val = nn_model.predict([X_val_temporal_reshaped, X_val_non_temporal])\n",
    "y_pred_val = [1 if prob > 0.5 else 0 for prob in y_pred_proba_val]\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1502/1502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step\n",
      "XG Boost\n",
      "Test Accuracy: 0.7282794221722659\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74     24076\n",
      "           1       0.75      0.69      0.72     23966\n",
      "\n",
      "    accuracy                           0.73     48042\n",
      "   macro avg       0.73      0.73      0.73     48042\n",
      "weighted avg       0.73      0.73      0.73     48042\n",
      "\n",
      "AUC:  0.7281888737072097\n",
      "Gradient Boosting Classifier\n",
      "Test Accuracy: 0.6973065234586403\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71     24076\n",
      "           1       0.72      0.65      0.68     23966\n",
      "\n",
      "    accuracy                           0.70     48042\n",
      "   macro avg       0.70      0.70      0.70     48042\n",
      "weighted avg       0.70      0.70      0.70     48042\n",
      "\n",
      "AUC:  0.6971944609961859\n",
      "Neural Network\n",
      "Test Accuracy: 0.7091295116772823\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.80      0.73     24076\n",
      "           1       0.75      0.62      0.68     23966\n",
      "\n",
      "    accuracy                           0.71     48042\n",
      "   macro avg       0.72      0.71      0.71     48042\n",
      "weighted avg       0.72      0.71      0.71     48042\n",
      "\n",
      "AUC:  0.7089204757135243\n"
     ]
    }
   ],
   "source": [
    "#Performance on Test Data\n",
    "\n",
    "test_dmatrix = xgb.DMatrix(X_test_pca)\n",
    "y_pred_proba_test = xgb_model.predict(test_dmatrix)\n",
    "y_pred_test_xgb = [1 if prob > 0.5 else 0 for prob in y_pred_proba_test]\n",
    "\n",
    "y_pred_test_gb = gb_model.predict(X_test_pca)\n",
    "\n",
    "X_test_temporal = X_test_pca[:, :temporal_features]  # Temporal features\n",
    "X_test_non_temporal = X_test_pca[:, temporal_features:]  # Non-temporal features\n",
    "X_test_temporal_reshaped = X_test_temporal.reshape((X_test_temporal.shape[0], 100, 2)) \n",
    "y_pred_proba_test = nn_model.predict([X_test_temporal_reshaped, X_test_non_temporal])\n",
    "y_pred_test_nn = [1 if prob > 0.5 else 0 for prob in y_pred_proba_test]\n",
    "\n",
    "print(\"XG Boost\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test_xgb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test_xgb))\n",
    "auc = roc_auc_score(y_test, y_pred_test_xgb)\n",
    "print(\"AUC: \", auc)\n",
    "\n",
    "print(\"Gradient Boosting Classifier\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test_gb))\n",
    "auc = roc_auc_score(y_test, y_pred_test_gb)\n",
    "print(\"AUC: \", auc)\n",
    "\n",
    "print(\"Neural Network\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test_nn))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test_nn))\n",
    "auc = roc_auc_score(y_test, y_pred_test_nn)\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60077, 3)\n",
      "      epitope             tcr  label\n",
      "0  EAAGIGILTV      CASSLGNEQF      0\n",
      "1  EAAGIGILTV   CASSLGVATGELF      0\n",
      "2   YLEPGPVTA   CASSLGSSYEQYF      0\n",
      "3   SAYGEPRKL   CASRLWFWALEAF      0\n",
      "4  EAAGIGILTV  CASSSGQGLNIQYF      0\n",
      "Distinct kmers: 729\n",
      "(60077, 1660)\n",
      "[0.99040645 0.9504607  0.9927874  0.4515879  0.61158174 0.9215012\n",
      " 0.87452716 0.32839414 0.17978652 0.5254405 ]\n"
     ]
    }
   ],
   "source": [
    "# Predict on hold-out set\n",
    "\n",
    "holdout_file_path = '../data/true_hold_out/tcr_split_test.csv'\n",
    "\n",
    "holdout_df = pd.read_csv(holdout_file_path, header=None)\n",
    "holdout_df.columns = ['epitope', 'tcr']\n",
    "\n",
    "holdout_df['epitope'] = holdout_df['epitope'].str.upper()\n",
    "holdout_df['tcr'] = holdout_df['tcr'].str.upper()\n",
    "\n",
    "# default value to satisfy preprocess function definition\n",
    "holdout_df['label'] = 0\n",
    "\n",
    "print(holdout_df.shape)\n",
    "print(holdout_df.head())\n",
    "\n",
    "X_holdout, y_holdout = data_preprocess(holdout_df, k=2, min_tcrs_amount=1)\n",
    "print(X_holdout.shape)\n",
    "\n",
    "X_holdout_scaled = scaler.transform(X_holdout)\n",
    "\n",
    "y_pred_proba_holdout = xgb_model.predict(xgb.DMatrix(X_holdout_scaled))\n",
    "# y_pred_holdout = [1 if prob > 0.5 else 0 for prob in y_pred_proba_holdout]\n",
    "\n",
    "print(y_pred_proba_holdout[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_df['label'] = y_pred_proba_holdout\n",
    "holdout_df.to_csv('../data/true_hold_out/tcr_split_test_pred.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
